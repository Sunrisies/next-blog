在[第 6 章](https://juejin.cn/book/7196580339181944872/section/7196589491266519080 "https://juejin.cn/book/7196580339181944872/section/7196589491266519080")我们讲过**代码的执行流程**，我们知道代码是被编译成机器指令，然后在 CPU 中一条一条被执行的。

那么，在这个过程中，计算机是怎么执行的呢？或者说，从代码被编写开始，到代码被运行出结果为止，这中间经历了什么呢？

接下来我们就来了解下。

另类的计算机定义
--------

我们都知道，计算机是由`控制器`、`运算器`、`存储器`、`输入设备`、`输出设备`这五部分组成的。

这太难记了，我们可以再宏观一些：**计算机是由`输入系统`、`输出系统`、`运算系统`这三部分组成的**。

站在程序员的角度来说，我把我的代码`输入`给计算机，计算机自己`计算`后，把结果`输出`给我，这就完事了。也就是说，我并不知道计算机内部发生了什么，我只知道我给它数据，它计算，算完把结果给我，所以，对于我来说，计算机就是个黑盒子，它就有三个功能：**接受输入、进行运算、输出结果**，如此而已。

这就不能叫冯·诺伊曼结构了，你可以叫它入算出结构，如果给点面子的话，你可以叫它奔波儿灞取经结构。

OK，到这里，我们就知道计算机的三个核心功能了：**输入、运算、输出**。

计算机的工作流程
--------

那么，计算机的工作流程是什么样的呢？比如，我给它一段代码，它是怎么干的呢？

从大方面看，就是三个步骤：

1.  读入你写的代码，也就是`输入`，至于怎么读，读到哪，先别管；
2.  执行读入的代码，也就是`运算`，至于怎么运算的，也先别管；
3.  将运算的结果展示出来，也就是`输出`，至于怎么展示的，也先别管。

好，大步骤我们知道了，我们就来看下每个小步骤是怎么执行的。

### 输入

现在我打开编辑器写了如下代码：

    public class Hello {
        public void sayHello() {
            System.out.println("hello, world");
        }
    }
    

然后点击运行，结果就出来了。

那么，这块代码是怎么被读入计算机的呢？

首先，我们知道，我们写的这块代码是个后缀名为`.java`的文件，存放在我们电脑上的磁盘中。而代码要执行的时候，就会把它读入内存中，怎么读入呢？你可以理解为就跟普通的文件读写一样，通过 IO 操作读入内存中。好，现在我们的代码已经从磁盘中到内存中了，也就是已经完成了**输入**阶段了。

> Tips：Java 代码具有跨平台性，`.java` 文件先被编译成`.class`文件，然后读入到 JVM 中，而 JVM 就是运行在内存中的。

接下来，我们来看运算的过程。

### 运算

现在，我们的代码已经在内存中了，接下来，我们就要开始运算了。

[第 6 章](https://juejin.cn/book/7196580339181944872/section/7196589491266519080 "https://juejin.cn/book/7196580339181944872/section/7196589491266519080")我们说过，计算机执行的是机器码，而我们读入的并不是机器码啊，这怎么可能执行呢？

所以，我们运算的第一步，就是将非机器码转换为机器码，也就是我们写的代码，转换为 0110 这种二进制。其实，**代码是先经过编译器转换为汇编语言，然后再经过汇编处理，才能变换为机器码**。

好，现在，我们的代码已经从自己写的语言变换为二进制了，那么接下来就是运算了。

还记得我们[第 6 章](https://juejin.cn/book/7196580339181944872/section/7196589491266519080 "https://juejin.cn/book/7196580339181944872/section/7196589491266519080")讲的机器指令吗，没错，这里的运算就要用到那些指令了。

我们得到的机器码，里面就包含这些指令，以及这些指令要操作的数，那么，计算机就用这些指令来操作对应的数字，就得到了最后的结果，那么接下来，就只需要把这些结果输出就行了。

等等等等，你这不对啊，你说的是单纯的数字计算和打印，比如我要是写个页面呢？页面最后得显示出来吧，那页面也是输出啊，这页面是怎么计算的？

好，我们就来看`页面（以下统称为 UI）`的计算过程。

首先，不管你是 UI 还是非 UI，你的输入过程都是一样的，都是将自己写的文件读入内存，然后转换为机器码，这个就不废话了。

对于 UI 来说，我们把 UI 当成一块块的 UI 单元，也就是一个个的矩形块，这些矩形块有的显示文字，有的显示图像，有的只是单纯地用来排版其他矩形块，这个应该都能理解。

不管是什么样的 UI 单元，其核心属性就是这么几点：

*   `尺寸`，也就是宽高；
*   `位置`，也就是自己在屏幕上的坐标；
*   `内容`，也就是自己要显示的内容。

那么，UI 单元的内容是啥子呢？

其实就是**像素点**！我们的 UI 单元是一个个的像素点组成的，可以理解为一个个的彩色小点，这些点都存储着自己的颜色值，是一个 32 位数字，高 8 位存储透明度值，接下来 8 位存放红色值，再接下来 8 位存放绿色植，最后 8 位存放蓝色值，所以也叫 ARGB。

所以，我们的 UI 就像一张大纸，从上到下密密麻麻地铺满了很多小点，我们 UI 的绘制其实就是遍历一个个的小点，然后将对应的颜色取出来画到屏幕上而已。

没错，正是这样！

那这是谁画的呢？屏幕！

那你运算器运算了啥啊？

我们写的 UI 肯定不是写死的坐标，而是使用各种不同的布局，然后加上各种各样的内边距（padding）、外边距（margin）等来排版，这就是运算器要运算的了。

再比如，我们写的宽高等不是固定值，而是包括内容、百分比等，这些也是 UI 要计算的。

它会将我们写的这些边距、尺寸等，计算成复合当前屏幕的固定值，然后保存起来，交给屏幕去渲染。

> Tips：其实图形的计算，不是 CPU 干的，而是 GPU 干的，因为 CPU 不适合做图像计算。

所以，你 CPU 很屌但是玩游戏卡，就是 GPU 不行，因为游戏需要大量的图像计算，所以需要很屌的 GPU，也就是需要个好显卡。

不管是谁算的，总之都是计算机算的，计算机将 UI 数据计算好后，会保存起来，保存到一个 Bitmap 上，或者说保存到一个矩阵中，然后交给屏幕去绘制。

接下来就是输出了。

### 输出

输出也分为两种：UI 类型的和非 UI 类型的。

`非 UI 类型`的比较简单，我们直接从内存读出数据后，打印出来即可。或者直接将结果写到电脑文件上即可，这就意味着数据又从内存中回到磁盘中了。

那么，`UI 类型`的呢？就比较费劲了，我们的屏幕需要定时从内存中读出数据，然后将其绘制出来，那为啥不是一次绘制呢？因为一次干不完啊。

你这么理解，我们的 UI 数据在运算之后，保存到了矩阵中，我们的屏幕就定期从这个矩阵中从上到下，一行一行地读出数据，也就是像素点，然后将其绘制在屏幕上，这样的一屏幕数据，叫做**一帧**。

那么，这个定期时间是多长呢？

就看你的**屏幕刷新率**了，比如，如果你的屏幕刷新率是 60HZ，那就等于 1 秒内（1000ms）要绘制 60 次，也就是每 16ms 就要绘制一次（1000/60=16），所以这个定时时间就是 16ms；如果你的屏幕刷新率是 120HZ，那就等于 1 秒内绘制 120 次，所以每 8ms 就要绘制一次。

所以，这个时间取决于屏幕刷新率。

有人说，这 1 秒要绘制 60 次，那就意味着 CPU 在 1 秒内要计算 60 次啊，那它后面的数据不会把前面的覆盖了吗？

比如，CPU 计算了第一帧，屏幕去绘制了，还没绘制完，CPU 就计算出了第二帧，然后把第一帧覆盖了，这不就乱了吗？

没错，这样会乱，这个就叫做屏幕的**撕裂**。

所以，我们就开辟了**缓存**，CPU 计算完一帧数据后，直接放在缓存中即可，屏幕画完就从缓存中取出下一条接着画。

那你这缓存多大呢？比如你设置为 10，CPU 可能都计算了 11 条了，你这第 1 条还没画完，这不就还是覆盖吗？

对啊，所以问题的核心是啥呢？是 CPU 和屏幕自己干自己的，速度不同步。

你看，这是不是跟我们[第 17 章](https://juejin.cn/book/7196580339181944872/section/7196590941707173923 "https://juejin.cn/book/7196580339181944872/section/7196590941707173923")中讲的 TCP 的停止等待协议很像呢？此时 CPU 是发送方，屏幕是接收方，屏幕来不及处理，CPU 就发送下一条数据了。

所以怎么解决呢？

**停止等待协议**！

> Tips: 学会类比，举一反三，融会贯通，就能迅速扩大知识面，并且记得更牢。

OK，我们也来让 CPU 和屏幕停止等待一下！首先 CPU 绘制完后，就将数据放进缓冲区，然后停下来等着；我们再让屏幕每次绘制完后，给 CPU 发个信号，告诉 CPU：劳资画完了，你可以开始了。这个时候，CPU 就继续绘制，并将新数据放入缓存中。

那么，这个缓存的大小是多少呢？

因为我们有两个同时干活的，所以缓冲区大小给 `2` 就行了，这样可以保证他俩同时工作。这两个缓冲区一个叫`后缓冲，供 CPU 计算`；一个叫`前缓冲，供屏幕绘制`。

CPU 不断向后缓冲写入数据，屏幕不断从前缓冲取出数据绘制，绘制完就发送信号给 CPU 计算下一帧，CPU 接到信号后就交换前后缓冲数据，并接着计算下一帧数据。就这样循环往复，我们眼前就出现了不断变化的画面。

这个信号就叫做 **VSYNC**，也就是**垂直同步信号**。

总结
--

本章从宏观角度讲了计算机的工作流程：**输入->运算->输出**。

*   输入：将磁盘上的文件读入到内存中。
*   运算：将读入的数据转换为机器码并执行。
*   输出：将执行的结果输出到磁盘或屏幕上。

并且，我们重点讲了 UI 类型的运算和输出过程，这里面有个很重要的点：**发送方和接收方速率不同的情况下就采用停止等待协议**，这其实可以高度抽离一下，也就是**生产者消费者模型**，这是一个很重要的模型，比方说本章的 CPU 和屏幕的关系，[第 17 章](https://juejin.cn/book/7196580339181944872/section/7196590941707173923 "https://juejin.cn/book/7196580339181944872/section/7196590941707173923")的 TCP 协议中发送方和接收方的关系，还有[第 14 章](https://juejin.cn/book/7196580339181944872/section/7196591134489968692 "https://juejin.cn/book/7196580339181944872/section/7196591134489968692")讲到的多个线程之间的资源竞争，都是需要等待和唤醒的。

这就是`类比思想`，知识其实不多，就那么点，就看你是否能联想到一起，这一点我们会在后面的第 34 章专门讲解。那么下一章，我们就来看下计算机的内存设计和缓存设计是怎么实现的。